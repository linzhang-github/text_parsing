{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analyzing \n",
    "\n",
    "When analyzing the text,there are ususally three major perspectives are considered. \n",
    "- first is the quantity measures, including the number of words or characters, the number or numbers, the freuency of the word and etc.  \n",
    "- The other one the the text readability. text readability is an important factor to be considered, expecially for the quantative analysis study. The popular readability metrics include Flesch Kincaid Grade Level, Flesch Reading Ease, Gunning Fog Index, Dale Chall Readability, Automated Readability Index (ARI), Coleman Liau Index, Lisnear Write, and SMOG. Here's a package in python called **py-readability-metrics** which can enable us to get the score of all these mentioned metrics. \n",
    "- The third one is the content analysis of the text file, which LDA(laatent dirichlet allocaation method is a popular top  modeling used nowadays. \n",
    "\n",
    "\n",
    "## Installation \n",
    "~~~\n",
    "pip install nltk\n",
    "pip install py-readability-metrics\n",
    "python -m nltk.downloader punkt\n",
    "~~~\n",
    "\n",
    "## Basic Usage \n",
    "~~~\n",
    "from readability import Readability\n",
    "import nltk\n",
    "import string \n",
    "from nltk.corpus import stopwords  \n",
    "\n",
    "with open('sample.txt', 'r') as f: \n",
    "    text = f.read() \n",
    "r = Readability(text) \n",
    "print( \n",
    "r.flesch_kincaid(),\n",
    "r.flesch(),\n",
    "r.gunning_fog(),\n",
    "r.smog(),\n",
    "r.coleman_liau(),\n",
    "r.dale_chall(),\n",
    "r.ari(),\n",
    "r.linsear_write(),\n",
    "r.smog(),sep = '\\n')\n",
    "\n",
    "# r.flesch().score\n",
    "# r.flesch().ease \n",
    "# r.flesch().grade_levels\n",
    "~~~ \n",
    "#### Reference:\n",
    "https://py-readability-metrics.readthedocs.io/en/latest/flesch_kincaid.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Read all the txt files in the subfolders and mail folders. \n",
    "create a list of all the txt files \n",
    "'''\n",
    "import pandas as pd\n",
    "from readability import Readability\n",
    "import nltk\n",
    "import string \n",
    "from nltk.corpus import stopwords  \n",
    "import os\n",
    "from pathlib import Path\n",
    "txt_list = []\n",
    "for root, dirs, files in os.walk(r'main_directory'):\n",
    "#     txt_list = [os.path.join(root,file) for file in files if file.endswith(\".txt\")]\n",
    "    for file in files: \n",
    "        if file.endswith('.txt'): \n",
    "            txt_list.append(os.path.join(root,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''There are 10 BatchEngine files which are GB2312 encoding which cannot be parsed. \n",
    "And they need to be removed from the txt_list first before parsing all the other files''' \n",
    "\n",
    "\n",
    "import chardet  \n",
    "for file in txt_list: \n",
    "    rawdata = open(file, 'rb').read()\n",
    "    result = chardet.detect(rawdata)\n",
    "    charenc = result['encoding'] \n",
    "    if charenc == None: \n",
    "        print(file, charenc, file = open(r\"empty.txt\",\"a\")) \n",
    "    if charenc == 'GB2312': \n",
    "        print(file, charenc, file = open(r\"GB2312.txt\",\"a\"))\n",
    "\n",
    "with open('GB2312.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "batchengine_ls = [line.split(\" \")[0] for line in lines]\n",
    "\n",
    "## Remove batchengine ls from txt_list  \n",
    "txt_list_clean = list(set(txt_list) - set(batchengine_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build up function to get all these 3 metrics for readability measures: \n",
    "1. flesch_kincaid\n",
    "2. flesch_read_ease \n",
    "3. fog index \n",
    "'''\n",
    "import glob \n",
    "ls_txt_name = [names.split(\"main directory of text files\")[1] for names in txt_list_clean] \n",
    "\n",
    "def get_rd_metric(filename): \n",
    "    try: \n",
    "        with open(filename,'r') as f: \n",
    "            text = f.read()\n",
    "            r = Readability(text)\n",
    "        \n",
    "            return r.flesch_kincaid().score,  r.flesch_kincaid().grade_level, \\\n",
    "                    r.flesch().score, r.flesch().grade_levels, \\\n",
    "                    r.gunning_fog().score, r.gunning_fog().grade_level \n",
    "    except Exception as e: \n",
    "        print(filename,e, file = open(r\"error_log_0906.txt\",\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all the readability metrics in a list \n",
    "\n",
    "ls_all_metric = [get_rd_metric(filename) for filename in txt_list_clean] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe from the readability metrics list \n",
    "\n",
    "dic_metric = {\"file_name\": ls_txt_name,\"metric\":ls_all_metric}\n",
    "df_metric = pd.DataFrame(dic_metric)\n",
    "\n",
    "\n",
    "df_metric[['flesch_kincaid_score','flesch_kincaid_grade_level',\n",
    "           'flesch_read_ease_score','flesch_read_ease_grade_level',\n",
    "         'fog_score','fog_grade_level']] = pd.DataFrame(\n",
    "    df_metric['metric'].tolist(), index = df_metric.index) \n",
    "\n",
    "## save pandas to csv showing chinese character, using utf_8_sig encoding \n",
    "df_metric.to_csv('metric_table.csv', encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show full rows/full columns information\n",
    "\n",
    "pd.set_option('display.max_colwidth',-1)\n",
    "pd.set_option('display.max_rows',-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
